---
title: "Class 8: Unsupervised Learning Mini-Project"
author: "Yiyu"
format: pdf
toc: true

---
Today we will practice applying our PCA and clustering methods from the last class on some breast cancer FNA data.

Let's get the data into R...

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)

```
> Q1. How many samples/patients do we have?

There are `r nrow(wisc.df)` samples in this dataset.

> Q2. How many cancer/non-cancer diagnoses samples are there?

We can use the `table()` function. It is a super useful utility for counting up the number of observations of each type.

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many columns/dimensions are there in this data set?

```{r}
ncol(wisc.df)
dim(wisc.df)
```

> Q4. How many columns are suffixed with "_mean"?

```{r}
colnames(wisc.df)
x <- grep("_mean", colnames(wisc.df))
length(x)
```
## Let's tidy our dataset to remove diagnosis.
We will not be using the diagnosis column for our unsupervised analysis as it tells us the answer. So let's save it as a vector first, then remove it from the data frame so the data can undergo clusering, PCA, etc...

```{r}
diagnosis <- wisc.df$diagnosis
```

```{r}
wisc.data <- wisc.df[,-1]
```


## Cluster the dataset

Let's try a `hclust()`. 

```{r}
hc.raw <- hclust(dist(wisc.data))
plot(hc.raw)
```

To get some clusters out of this, I can "cut" the tree at a given height:

```{r}
grps <- cutree(hc.raw, h=4000)
table(grps)
```

To see the correspondance of our cluster `grps` with the expert `diagnosis`, I can use `table()` again.

```{r}
table(grps, diagnosis)
```

That is not that useful of a clustering result...

## Principal Component Analysis (PCA)

Scaling data before analysis is often critical.

Side-note: The default behavior for `prcomp()` is `scale=False`

The is a dataset in R called `mtcars()` which has loads of numbers about old cars.


```{r}
head(mtcars)

```

```{r}
colMeans(mtcars)
```
```{r}
apply(mtcars, 2, sd)
```

```{r}
pc.noscale <- prcomp(mtcars, scale = FALSE)
pc.scale <- prcomp(mtcars, scale = TRUE)
```

Let's look at the loadings first:
```{r}
pc.noscale$rotation
```
Plot the noscale version
```{r}
library(ggplot2)
ggplot(pc.noscale$rotation) + 
  aes(PC1, rownames(pc.noscale$rotation)) +
  geom_col()
```
We see the representation is heavily dominated by hp and disp because it naturally has larger numbers.

Now plot the scaled version
```{r}
library(ggplot2)
ggplot(pc.scale$rotation) + 
  aes(PC1, rownames(pc.scale$rotation)) +
  geom_col()
```
We see the representation is much more evenly distributed.

The main PC result figure is often alled a "score plot" or "PC plot" or "PC1 vs PC2 plot"

```{r}
ggplot(pc.noscale$x) +
  aes(PC1, PC2, label = rownames(pc.noscale$x)) +
  geom_point() +
  geom_label()
```

```{r}
ggplot(pc.scale$x) +
  aes(PC1, PC2, label = rownames(pc.scale$x)) +
  geom_point() +
  geom_label()
```

```{r}
x <- scale(mtcars)
round(colMeans(x))
round(apply(x, 2, sd))
```

> **Key-point**: Generally we want to "scale" our data before analysis to avoid being mis-lead due to your data having different measurement units.

## Breast Cancer PCA

We will scale our data.
```{r}
pca <-  prcomp(wisc.data, scale = T)
```

See how well we are doing:

```{r}
summary(pca)
```

Our PC plot

```{r}
ggplot(pca$x) +
  aes(PC1, PC2, col = diagnosis) + 
  geom_point() +
  xlab("PC1 (44.3%)") +
  ylab("PC2 (18.9%)")
```

> Q4. How many PCs capture 80% of the original variance in the dataset?

```{r}
summary(pca)

```
Need 5 PCs to capture at least 80%.

```{r}
plot(pca)
```

> Q5. Use ggplot to plot a "screen-plot: of the variance per PC.

```{r}
attributes(pca)
```

We can extract the sdev ad figure out the variance...

```{r}
v <- pca$sdev^2
round(v)
sum(v)
```

The portion of variance captured in each PC. See how it is the same as the proportion variance row of the summary pca table.

```{r}
round(v/sum(v), 4)
```

See how it is the same as the cumulative sum row of the summary pca table.
```{r}
which(cumsum(v/sum(v)) > 0.8)
```

```{r}
library(factoextra)
fviz_eig(pca, addlabels = TRUE)
```

## Combine PCA and clustering

We saw earlier that clustering the raw data alone did not provide useful results. 

We can use our new PC variables (our PCs) as a basis for clustering. Use our `$x` PC scores. Don't include all PCs, it would be the same as previously done `hc.raw`. Let's cluster in PC1-2 subspace.

```{r}
hc.pca <- hclust(dist(pca$x[,1:2]), method = "ward.D2")
plot(hc.pca)
abline(h=60, col="blue")
```

> Q6. Does your clustering help separate cancer from non-cancer diagnoses (i.e. M vs B.)?

```{r}
grps2 <- cutree (hc.pca, h= 60)
crosstable <- table(grps2, diagnosis)
```

Positive cancer samples "M" 
Negative non-cancer samples "B"

True is our cluster/group 1
False is our cluster/group 2

> Q7. How many true positives (TP) do we have?

> Q8. How many false positives (FP) do we have?

Sensitivity: TP/(TP+FN)

Specificity: TN/(TN+FN)

## Prediction with our PCA model

We can take new data (in this case from UofM) and project it onto our new variables (PCs)

Read the UofM data
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
```

Projection
```{r}
npc <- predict(pca, newdata=new)
npc
```

Base R plot
```{r}
plot(pca$x[,1:2], col=grps2)

#Now I will add the new points from UofM data
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
We should follow up on patient 2.
