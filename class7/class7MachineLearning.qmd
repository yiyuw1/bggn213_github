---
title: "Class 7: Machine Learning 1"
author: "Yiyu"
format: pdf
---

Today we will delve into unsupervised machine learning with an initial focus on clusering and dimensionality reduction. 

Let's start by making up some data to cluster:
The `rnorm`()` function can help us here...

```{r}
hist( rnorm(3000, mean = 3) )
```

Let's get some data centered at 3,-3 and -3,3
```{r}
#Combine 30 +3 values with 30 -3 values
x <- c( rnorm(30, mean = 3), rnorm(30, mean = -3) )

#Bind these values together 
z <- cbind(x=x,y=rev(x))

head(z)
```

```{r}
plot(z)
```
##K-means
Now we can see how K-means clusters this data. The main function for K-means clustering in "base R" is called `kmeans()`.

```{r}
km <- kmeans(z, centers = 2)
km
```

> Q. What size is each cluster?

```{r}
km$size
```
> Q. The cluster membership vector (i.e. the answer cluster to which each point is allocated)

```{r}
km$cluster
```

> Q. Cluster centers 

```{r}
km$centers

```
> Q. Make a results figure , i.e plot the data `z` colored by cluster membership and show cluster centers.

```{r}
plot(z, col = c("blue", "red"))

```
Uses recycling, so it will mix colors within clusters. 
You can color based on a number, which will be useful here.

```{r}
plot(z, col = 1)
```
We know `km$cluster` categorizes clusters into 1 or 2, so color by `km$cluster` will color the clusters with separate colors.

```{r}
plot(z, col = km$cluster)
```
Can also color cluster center
```{r}
plot(z, col = km$cluster)
points(km$center, col = "blue", pch=15)
```

> Q. Re-run your K-means clustering and ask for 4 clusters and plot the results as above.

```{r}
km4 <- kmeans(z, centers=4)
plot(z, col=km4$cluster)
points(km4$center, col = "blue", pch=15)
```

##Hierarchial Clustering

The main "base R" function for this is `hclust()`. Unlike `kmeans()` you can't just give your dataset as input, you need to provide a distance matrix.

We can use the `dist()` function for this

```{r}
d <-dist(z)
```

```{r}
dim(z)
```
```{r}
hc <- hclust(d)
hc
```
There is a custom plot() for hclust objects, let's see it.

```{r}
plot(hc)
abline(h=8, col="red")
```

The function to extract clusters/grps from an hclust object/tree is called `cutree()`:

```{r}
grps <- cutree(hc, h=8)
grps
```

> Q. Plot data with hclust clusters:

```{r}
plot(z, col = grps)
```

##Principal Component Analysis (PCA)

The main function for PCA in base R for PCA is called `prcomp()`. There are many, many add on packages with PCA functions tailored to particular data types (RNASeq, protein structures, metagenomics. etc...)

##PCA of UK food data

Read the data into R, it is a CSV file and we can use `read.csv()` to read it:

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
x
```

I would like the food names as row names, not their own column of data (first column currently). I can fix this like so:
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
```

Could be risky, if you run it multiple times you will continue to delete the first column.

A better way to do this is to do it at the time of data import with `read.csv()`.

```{r}
food <- read.csv(url, row.names=1)
food
```

Let's make some plots and dig into the data a little.

```{r}
rainbow(nrow(food))
```
```{r}
barplot(as.matrix(food), beside=T, col=rainbow(nrow(food)))
```

```{r}
barplot(as.matrix(t(food)), beside=T, col=rainbow(nrow(food)))
```

How about a so-called "pairs" plot where we plot each country against all other countries.

```{r}
pairs(food, col=rainbow(nrow(food)), pch=16)
```

Really there has to be a better way....

##PCA to the rescue

We can run a Principal Component Analysis (PCA) for this data with the `prcomp()` function.

```{r}
head(food)
```

We need to take the transpose of this data to get the foods in the column and the countries in the rows.

```{r}
pca <- prcomp(t(food))
summary(pca)
```

What is in my `pca` resultobject?

```{r}
attributes(pca)
```

The scores along the new PCs 
```{r}
pca$x
```

To make my main result figure, often called a PC plot (or score plot, ordenation plot, PC1 vs PC2 plot etc.)

```{r}
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab= "PC2",
     col = c("orange", "red","blue","darkgreen"), pch=16)
```
Try with ggplot 2
```{r}
library(ggplot2)

data <- as.data.frame(pca$x)

ggplot(data) +
  aes(PC1, PC2) +
  geom_point(col = c("orange", "red","blue","darkgreen"))
```

To see the contributions of the original variables (foods) to these new PCs we can look at the `pca$rotation` component of our results object.

```{r}
loadings <- as.data.frame(pca$rotation)
loadings$name <- rownames(loadings)

ggplot(loadings) +
  aes(PC1, name) +
  geom_col()
```




